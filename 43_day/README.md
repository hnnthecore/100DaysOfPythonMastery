# âš¡ Day 43 â€“ AsyncCrawler: Parallel Web Scraper

Speed isn't magic â€” it's concurrency executed with intent.

---------------------------------------------------------

## ğŸ§  Concepts Practised
- Asynchronous scraping using aiohttp
- Parallel fetching using asyncio
- Fast multi-page extraction
- Parsing HTML with BeautifulSoup
- Exporting data to JSON & CSV
- Scaling scrapers for automation

---------------------------------------------------------

## ğŸ’¡ Project Overview

AsyncCrawler scrapes multiple pages at once instead of sequentially.  
Target: https://quotes.toscrape.com/page/1/

You set page count inside main.py:
    PAGES = 10

The scraper:
- Builds URLs automatically
- Fetches all pages in parallel
- Extracts quote + author + tags
- Saves output to:
    async_quotes.json
    async_quotes.csv

---------------------------------------------------------

## âš™ Features
â€¢ Scrapes many pages simultaneously  
â€¢ Much faster than requests-based scraping  
â€¢ Clean structured JSON output  
â€¢ CSV export for Excel/Sheets  
â€¢ Scales to 20/50/100+ pages easily  
â€¢ No Selenium needed

---------------------------------------------------------

## ğŸ“¸ Sample output

![output](https://raw.githubusercontent.com/hnnthecore/100DaysOfPythonMastery/refs/heads/main/assets/day43_output.png)

---------------------------------------------------------

## ğŸš€ How to Run

Install dependencies:
    pip install aiohttp beautifulsoup4

Run script:
    python main.py

Generated files:
    async_quotes.json
    async_quotes.csv

---------------------------------------------------------

## ğŸ“ Notes
â€¢ Async crawler = massive speed improvement  
â€¢ Ideal for pagination scraping  
â€¢ Next step before anti-blocking + rotation

---------------------------------------------------------

## ğŸ¯ Takeaways
You now know how to:
â€¢ Run async scrapers  
â€¢ Fetch data in parallel  
â€¢ Export datasets cleanly  
â€¢ Scale scrapers beyond Day 42

